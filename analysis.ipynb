{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid Mobility Data Analysis for Trend Prediction\n",
    "## Carson Woods, VZL837\n",
    "## CPSC 4180 Fall 2020 Final Project\n",
    "\n",
    "### Description:\n",
    "Statistical Analysis of Mobility Data and its impact on the rate of new COVID-19 Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from multiprocessing import Process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Apple Mobility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./data/applemobilitytrends-2020-09-21.csv does not exist: './data/applemobilitytrends-2020-09-21.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-89075516acfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in Apple Mobility Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m apple_data = pd.read_csv('./data/applemobilitytrends-2020-09-21.csv',\n\u001b[0m\u001b[1;32m      3\u001b[0m                          low_memory=False)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./data/applemobilitytrends-2020-09-21.csv does not exist: './data/applemobilitytrends-2020-09-21.csv'"
     ]
    }
   ],
   "source": [
    "# Read in Apple Mobility Data\n",
    "apple_data = pd.read_csv('./data/applemobilitytrends-2020-09-21.csv',\n",
    "                         low_memory=False)\n",
    "\n",
    "\n",
    "# Extract column names to be renamed\n",
    "apple_date_columns = apple_data.loc[:, '1/13/2020':]\n",
    "column_names = apple_date_columns.columns\n",
    "updated_column_names = []\n",
    "\n",
    "# Convert column names to have matching date format\n",
    "for name in column_names:\n",
    "    date = datetime.strptime(name, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "    updated_column_names.append(date)\n",
    "\n",
    "# Update names and reform original DataFrame\n",
    "apple_date_columns.columns = updated_column_names\n",
    "apple_data = pd.concat([apple_data.loc[:, :'country'], apple_date_columns],\n",
    "                       axis=1)\n",
    "\n",
    "# Forcibly clean up duplicate date columns to preserve memory\n",
    "del apple_date_columns\n",
    "del column_names\n",
    "del updated_column_names\n",
    "\n",
    "# Break the data into more specific subsets.\n",
    "# The data has the following structure (from broad to specific):\n",
    "# Country/Region -> Sub-Region(States in the US) -> County -> City\n",
    "apple_countries = apple_data.loc[apple_data['geo_type'] == 'country/region']\n",
    "apple_sub_regions = apple_data.loc[apple_data['geo_type'] == 'sub-region']\n",
    "apple_counties = apple_data.loc[apple_data['geo_type'] == 'county']\n",
    "apple_cities = apple_data.loc[apple_data['geo_type'] == 'city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apple_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Google Mobility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Google Data Informal Documentation\n",
    "\"\"\"\n",
    "\n",
    "# Read in Google Mobility Data\n",
    "google_data = pd.read_csv('./data/Google_Global_Mobility_Report.csv',\n",
    "                          low_memory=False).fillna(0)\n",
    "google_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load John Hopkins COVID-19 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in JHU time series data\n",
    "jhu_path = './data/COVID-19/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "jhu_data = pd.read_csv(jhu_path + 'time_series_covid19_confirmed_global.csv')\n",
    "\n",
    "# Rename US to United States in JHU Time Series df\n",
    "jhu_data.loc[jhu_data[\"Country/Region\"] == \"US\", \"Country/Region\"] = \"United States\"\n",
    "\n",
    "# Extract column names to be renamed\n",
    "# Date format does not match other data, so do conversion\n",
    "jhu_date_columns = jhu_data.loc[:, '1/22/20':]\n",
    "column_names = jhu_date_columns.columns\n",
    "updated_column_names = []\n",
    "\n",
    "# Convert column names to have matching date format\n",
    "for name in column_names:\n",
    "    date = datetime.strptime(name, '%m/%d/%y').strftime('%Y-%m-%d')\n",
    "    updated_column_names.append(date)\n",
    "\n",
    "# Update names and reform original DataFrame\n",
    "jhu_date_columns.columns = updated_column_names\n",
    "jhu_data = pd.concat([jhu_data.loc[:, :'Long'],\n",
    "                     jhu_date_columns],\n",
    "                     axis=1)\n",
    "\n",
    "# Forcibly clean up duplicate date columns to preserve memory\n",
    "del jhu_date_columns\n",
    "del column_names\n",
    "del updated_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Apple dataset into dataframes (one per country)\n",
    "This will allow for analysis to be done for every country more easily.\n",
    "Each dataframe will follow the same structure and format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# List of DataFrames for each country\n",
    "country_df_list = []\n",
    "\n",
    "# Gets all countries in Apple's dataset\n",
    "for index, row in apple_countries.iterrows():\n",
    "    country_name = row['region'].strip()\n",
    "\n",
    "    # Flag for determining if matching country dataframe was found\n",
    "    found = False\n",
    "\n",
    "    # Iterates through list of country dataframes\n",
    "    for index, df in enumerate(country_df_list):\n",
    "        # Checks to determine if country is already present\n",
    "        if df['region'].iloc[0].strip() == country_name:\n",
    "            modified_df = country_df_list[index].append(row,\n",
    "                                                        ignore_index=True)\n",
    "            country_df_list[index] = modified_df\n",
    "            found = True\n",
    "\n",
    "    # Ensures that countries that were not already found are added\n",
    "    if not found:\n",
    "        country_df_list.append(row.to_frame().T)\n",
    "\n",
    "\n",
    "# Converts the \"direction type\" index label to be a more general \"datatype\"\n",
    "# This now indicates whether it was walking, driving, transit, or covid\n",
    "# Where covid data is JHU time series data, and all other data is apple maps\n",
    "# mobility statistics.\n",
    "for index, df in enumerate(country_df_list):\n",
    "    df.columns = ['datatype' if x == 'transportation_type'\n",
    "                  else x for x in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Google data to the previosuly generated list of country dataframes\n",
    "Iterates through Google's data and adds data for each country to the corresponding dataframe. \n",
    "If the country isn't found, the data is disregarded since I want to do analysis on country's with multiple data sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Google country data into dataframe\n",
    "for country in set(google_data['country_region'].to_list()):        \n",
    "    \n",
    "    # Second line ensures that no duplicate city data from countries is picked up\n",
    "    country_data = google_data.loc[google_data['country_region'] == country]\n",
    "    country_data = country_data.loc[country_data['sub_region_1'] == 0]\n",
    "\n",
    "    # Seperates description information from mobility data\n",
    "    # temp: stores mobility data\n",
    "    # country_data: stores description information\n",
    "    temp = country_data.transpose().iloc[7:]\n",
    "    country_data = country_data.transpose().iloc[:7]\n",
    "    country_data = country_data.iloc[:,:6]\n",
    "    country_data = country_data.transpose()\n",
    "\n",
    "    # creates a single column dataframe\n",
    "    # will be used to label dataframe within country dataframe\n",
    "    datatypes = temp.index.values.tolist()\n",
    "    datatypes = pd.DataFrame(datatypes, columns=['datatype'])\n",
    "\n",
    "    # renames column index in temp to use date format\n",
    "    # renames row indices to be numeric\n",
    "    # this makes concatenation work later\n",
    "    temp.columns = temp.iloc[0]\n",
    "    temp = temp.drop(temp.index[0])\n",
    "    temp.index = list(range(6))\n",
    "    datatypes = datatypes.drop(datatypes.index[0])\n",
    "    datatypes.index = list(range(6))\n",
    "\n",
    "    # creates country dataframe with all information\n",
    "    # additional logic is needed to match overall column index format\n",
    "    google_country_df = pd.concat([country_data, datatypes, temp], axis=1)\n",
    "    google_country_df.rename(columns={'country_region_code':'geo_type',\n",
    "                                      'country_region':'region',\n",
    "                                      'sub_region_1':'sub-region',\n",
    "                                      'sub_region_2':'country'}, inplace=True)\n",
    "\n",
    "    # reorder columns to match country_df\n",
    "    cols = list(google_country_df.columns.values)\n",
    "    cols_reorder = ['geo_type',\n",
    "                    'region',\n",
    "                    'datatype',\n",
    "                    'sub-region',\n",
    "                    'country']\n",
    "    cols = cols_reorder + cols[8:]\n",
    "    \n",
    "    google_country_df = google_country_df[cols].iloc[0:6]\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    google_country_df = google_country_df.fillna(0)\n",
    "    \n",
    "    \n",
    "    df = google_country_df.iloc[:, 5:]\n",
    "    \n",
    "    df = pd.concat([google_country_df.iloc[:, 0:6],\n",
    "                                   df.groupby(df.columns, axis=1).mean()],\n",
    "                                  axis=1)\n",
    "    \n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "    \n",
    "    # Normalize data to match apple dataset\n",
    "    numeric_cols = [col for col in df if df[col].dtype.kind != 'O']\n",
    "    df[numeric_cols] += 100\n",
    "    df['geo_type'] = \"country/region\"\n",
    "    df['region'] = country\n",
    "    google_country_df = df\n",
    "\n",
    "    # find matching country in country_df_list\n",
    "    # append google data to matching dataframe\n",
    "    for index, country_df in enumerate(country_df_list):\n",
    "        if country_df['region'].iloc[0].strip() == country.strip():\n",
    "            df = pd.concat([country_df, google_country_df], axis=0)\n",
    "            country_df_list[index] = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add JHU data to the previosuly generated list of country dataframes\n",
    "Iterates through JHU's data and adds data for each country to the corresponding dataframe. \n",
    "If the country isn't found, the data is disregarded since COVID data is required for any analysis to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds JHU data for each country into each country's dataframe\n",
    "for index, row in jhu_data.iterrows():\n",
    "    country_name = row['Country/Region'].strip()\n",
    "    subregion_name = str(row['Province/State']).strip()\n",
    "\n",
    "    # This step gets each row into a labeled format that is compatible\n",
    "    # with the dataframes in the country_df_list. This does not mean that the\n",
    "    # element counts will be compatible. Apple/Google are missing some days and\n",
    "    # JHU has more data available to it. The synchronization will\n",
    "    # need to be done in an additional for loop.\n",
    "    row = pd.concat([pd.Series(['country/region',\n",
    "                                row[1],\n",
    "                                'covid',\n",
    "                                subregion_name,\n",
    "                                country_name]),\n",
    "                    row['2020-01-22':'2020-09-21']],\n",
    "                    axis=0)\n",
    "\n",
    "    # Searches for matching country dataframe\n",
    "    for index, df in enumerate(country_df_list):\n",
    "        if df['region'].iloc[0].strip() == country_name:\n",
    "            if subregion_name == \"nan\":\n",
    "                new_index = ['geo_type',\n",
    "                             'region',\n",
    "                             'datatype',\n",
    "                             'sub-region',\n",
    "                             'country']\n",
    "                new_index.extend(list(row.index.values[5:]))\n",
    "                row.index = new_index\n",
    "                modified_df = country_df_list[index].append(row,\n",
    "                                                            ignore_index=True)\n",
    "                country_df_list[index] = modified_df\n",
    "\n",
    "# Filter out countries that are lacking covid data\n",
    "for index, df in enumerate(country_df_list):\n",
    "    try:\n",
    "        covid_data = df.loc[df['datatype'] == 'covid'].iloc[0].tolist()[5:]\n",
    "    except:\n",
    "        country_df_list.pop(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Step\n",
    "Creates a directory for results intially, then defines a functino for analysis to be completed in. \n",
    "Within the analysis function, a linear regression is done on each type of data (driving, walking, etc.) from each data source (Apple, Google) against the JHU COVID data.\n",
    "Results are written to text files in the results directory. \n",
    "After that, scatter plots are generated for each data type against the JHU COVID data, where the size of each point on the scatter plot corresponds to the average value for a week of mobilty data. \n",
    "These plots are then written to a directory for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that figures directory exists\n",
    "# If figure regeneration is needed,\n",
    "# it ensures that the storage directory is regenerated\n",
    "results_path = os.path.join(os.getcwd(), 'results')\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(os.path.join(os.getcwd(),'results'))\n",
    "\n",
    "for df in country_df_list:\n",
    "    # Ensures that NaN are set to 0\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # Store country name for labeling\n",
    "    country_name = df['region'][0].strip(\"' \")\n",
    "\n",
    "    # Ensures that there is a path for figures to be stored (per country)\n",
    "    country_path = os.path.join(results_path, country_name)\n",
    "    if not os.path.exists(country_path):\n",
    "        os.makedirs(country_path)\n",
    "\n",
    "    # Converts df rows to lists for easier operations\n",
    "    date_list = df.columns.values.tolist()[5:]\n",
    "    covid_data = df.loc[df['datatype'] == 'covid'].iloc[0].tolist()[5:]\n",
    "    driving_data = df.loc[df['datatype'] == 'driving'].iloc[0].tolist()[5:]\n",
    "    walking_data = df.loc[df['datatype'] == 'walking'].iloc[0].tolist()[5:]\n",
    "    if 'residential_percent_change_from_baseline' in df.values:\n",
    "        residential_data = df.loc[df['datatype'] == 'residential_percent_change_from_baseline'].iloc[0].tolist()[5:]\n",
    "        workplace_data = df.loc[df['datatype'] == 'workplaces_percent_change_from_baseline'].iloc[0].tolist()[5:]\n",
    "\n",
    "    driving_data = driving_data[:200]\n",
    "    walking_data = walking_data[:200]\n",
    "    covid_data = covid_data[:200]\n",
    "    if 'residential_percent_change_from_baseline' in df.values:\n",
    "        residential_data = residential_data[:200]\n",
    "        workplace_data = workplace_data[:200]\n",
    "\n",
    "    driving_model = LinearRegression()\n",
    "    walking_model = LinearRegression()\n",
    "\n",
    "    driving_model.fit(np.array(driving_data).reshape(-1, 1), np.array(covid_data))\n",
    "    walking_model.fit(np.array(walking_data).reshape(-1, 1), np.array(covid_data))\n",
    "\n",
    "    driving_model_test = sm.OLS(covid_data, sm.add_constant(driving_data)).fit()\n",
    "    walking_model_test = sm.OLS(covid_data, sm.add_constant(walking_data)).fit()\n",
    "\n",
    "    driving_score = driving_model.score(np.array(driving_data).reshape(-1, 1), np.array(covid_data))\n",
    "    walking_score = walking_model.score(np.array(walking_data).reshape(-1, 1), np.array(covid_data))\n",
    "\n",
    "    print(\"\\n\" + country_name + \": \\n\\tDriving: \" + str(driving_score) + \"\\n\\tWalking: \" + str(walking_score))\n",
    "    regression_results_file = open(country_path + \"/\" + country_name + \"_regression_performance.txt\", \"w+\")\n",
    "    regression_results_file.write(\"Driving Regression Performance: \" + str(driving_score) + \"\\n\" )\n",
    "    regression_results_file.write(\"Walking Regression Performance: \" + str(walking_score) + \"\\n\")\n",
    "    regression_results_file.write(\"Driving Regression Summary:\\n\" + str(driving_model_test.summary()))\n",
    "    regression_results_file.write(\"Walking Regression Summary:\\n\" + str(walking_model_test.summary()))\n",
    "\n",
    "\n",
    "    if 'residential_percent_change_from_baseline' in df.values:\n",
    "        residential_data = residential_data[:200]\n",
    "        workplace_data = workplace_data[:200]\n",
    "\n",
    "        residential_model = LinearRegression().fit(np.array(residential_data).reshape(-1, 1), np.array(covid_data))\n",
    "        workplace_model = LinearRegression().fit(np.array(workplace_data).reshape(-1, 1), np.array(covid_data))\n",
    "\n",
    "        residential_score = residential_model.score(np.array(residential_data).reshape(-1, 1), np.array(covid_data))\n",
    "        workplace_score = workplace_model.score(np.array(workplace_data).reshape(-1, 1), np.array(covid_data))\n",
    "\n",
    "        residential_model_test = sm.OLS(covid_data, sm.add_constant(residential_data)).fit()\n",
    "        workplace_model_test = sm.OLS(covid_data, sm.add_constant(workplace_data)).fit()\n",
    "\n",
    "        regression_results_file.write(\"Residential Regression Performance: \" + str(residential_score) + \"\\n\")\n",
    "        regression_results_file.write(\"Workplace Regression Performance: \" + str(workplace_score) + \"\\n\")\n",
    "        regression_results_file.write(\"Residential Regression Summary:\\n\" + str(residential_model_test.summary()))\n",
    "        regression_results_file.write(\"Workplace Regression Summary:\\n\" + str(workplace_model_test.summary()))\n",
    "\n",
    "    regression_results_file.close()\n",
    "\n",
    "\n",
    "    # The data was initially too messy to interpret, and without\n",
    "    # normalization it was useless. This takes average values over 7 day\n",
    "    # intervals to make the data significantly more readable\n",
    "    walking_means = []\n",
    "    driving_means = []\n",
    "    labels = []\n",
    "    covid = []\n",
    "\n",
    "    if 'residential_percent_change_from_baseline' in df.values:\n",
    "        residential_means = []\n",
    "        workplace_means = []\n",
    "\n",
    "    for x in range(0, int(len(covid_data)/7)):\n",
    "        walking_mean = 0\n",
    "        driving_mean = 0\n",
    "        workplace_mean = 0\n",
    "        residential_mean = 0\n",
    "        for i in range(0, 6):\n",
    "            walking_mean += walking_data[(x*7)+i]\n",
    "            driving_mean += driving_data[(x*7)+i]\n",
    "            if 'residential_percent_change_from_baseline' in df.values:\n",
    "                workplace_mean += workplace_data[(x*7)+i]\n",
    "                residential_mean += residential_data[(x*7)+i]\n",
    "            if i == 4:\n",
    "                covid.append(covid_data[(x*7)+i])\n",
    "        walking_mean /= 7\n",
    "        driving_mean /= 7\n",
    "        if 'residential_percent_change_from_baseline' in df.values:\n",
    "            workplace_mean /= 7\n",
    "            residential_mean /= 7\n",
    "\n",
    "        walking_means.append(walking_mean)\n",
    "        driving_means.append(driving_mean)\n",
    "        if 'residential_percent_change_from_baseline' in df.values:\n",
    "            workplace_means.append(workplace_mean)\n",
    "            residential_means.append(residential_mean)\n",
    "\n",
    "        # Labels are now number of days since start of data\n",
    "        labels.append((x*7)+4)\n",
    "\n",
    "    # Draw Plots for each country's respective walking and driving data\n",
    "    # Plots are scatter plots with, the size of each data point on the graph\n",
    "    # corresponding to the amount of directions requested that day\n",
    "    # (larger dots == more directions, smaller == less)\n",
    "    fig, ax0 = plt.subplots()\n",
    "    ax0.set_xscale('linear')\n",
    "    ax0.ticklabel_format(useOffset=False, style='plain')\n",
    "    ax0.scatter(labels, covid, s=walking_means)\n",
    "    fig.suptitle(country_name + \": Correlation of Walking Directions and Confirmed COVID Cases\")\n",
    "    ax0.set_xlabel(\"Time Passed In Days Since Jan 22nd\")\n",
    "    ax0.set_ylabel(\"Confirmed Covid Cases\")\n",
    "    file_name = country_name + '_covid_walking.png'\n",
    "    fig.savefig(os.path.join(country_path, file_name))\n",
    "    plt.clf()\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig, ax0 = plt.subplots()\n",
    "    ax0.set_xscale('linear')\n",
    "    ax0.ticklabel_format(useOffset=False, style='plain')\n",
    "    ax0.scatter(labels, covid, s=driving_means)\n",
    "    fig.suptitle(country_name + \": Correlation of Driving Directions and Confirmed COVID Cases\")\n",
    "    ax0.set_xlabel(\"Time Passed In Days Since Jan 22nd\")\n",
    "    ax0.set_ylabel(\"Confirmed Covid Cases\")\n",
    "    file_name = country_name + '_covid_driving_apple.png'\n",
    "    fig.savefig(os.path.join(country_path, file_name))\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "\n",
    "    if 'residential_percent_change_from_baseline' in df.values:\n",
    "        fig, ax0 = plt.subplots()\n",
    "        ax0.set_xscale('linear')\n",
    "        ax0.ticklabel_format(useOffset=False, style='plain')\n",
    "        ax0.scatter(labels, covid, s=residential_means)\n",
    "        fig.suptitle(country_name + \": Correlation of Residential Directions and Confirmed COVID Cases\")\n",
    "        ax0.set_xlabel(\"Time Passed In Days Since Jan 22nd\")\n",
    "        ax0.set_ylabel(\"Confirmed Covid Cases\")\n",
    "        file_name = country_name + '_covid_residential_google.png'\n",
    "        fig.savefig(os.path.join(country_path, file_name))\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "        fig, ax0 = plt.subplots()\n",
    "        ax0.set_xscale('linear')\n",
    "        ax0.ticklabel_format(useOffset=False, style='plain')\n",
    "        ax0.scatter(labels, covid, s=workplace_means)\n",
    "        fig.suptitle(country_name + \": Correlation of Workplace Directions and Confirmed COVID Cases\")\n",
    "        ax0.set_xlabel(\"Time Passed In Days Since Jan 22nd\")\n",
    "        ax0.set_ylabel(\"Confirmed Covid Cases\")\n",
    "        file_name = country_name + '_covid_workplace_google.png'\n",
    "        fig.savefig(os.path.join(country_path, file_name))\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
