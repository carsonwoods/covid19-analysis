{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from multiprocessing import Process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Preprocessing #\n",
    "#################\n",
    "\n",
    "\"\"\"\n",
    "    Apple Data Informal Documentation\n",
    "\n",
    "    Rows: Various Categorical Information\n",
    "\n",
    "    Breaks geographical data into\n",
    "        - Countries\n",
    "        - Sub Regions: Such as states\n",
    "        - Counties\n",
    "        - Cities\n",
    "\"\"\"\n",
    "# Read in Apple Mobility Data\n",
    "apple_data = pd.read_csv('./data/applemobilitytrends-2020-09-21.csv',\n",
    "                         low_memory=False)\n",
    "\n",
    "\n",
    "# Extract column names to be renamed\n",
    "apple_date_columns = apple_data.loc[:, '1/13/2020':]\n",
    "column_names = apple_date_columns.columns\n",
    "updated_column_names = []\n",
    "\n",
    "# Convert column names to have matching date format\n",
    "for name in column_names:\n",
    "    date = datetime.strptime(name, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "    updated_column_names.append(date)\n",
    "\n",
    "# Update names and reform original DataFrame\n",
    "apple_date_columns.columns = updated_column_names\n",
    "apple_data = pd.concat([apple_data.loc[:, :'country'], apple_date_columns],\n",
    "                       axis=1)\n",
    "\n",
    "# Forcibly clean up duplicate date columns to preserve memory\n",
    "del apple_date_columns\n",
    "del column_names\n",
    "del updated_column_names\n",
    "\n",
    "# Break the data into more specific subsets.\n",
    "# The data has the following structure (from broad to specific):\n",
    "# Country/Region -> Sub-Region(States in the US) -> County -> City\n",
    "apple_countries = apple_data.loc[apple_data['geo_type'] == 'country/region']\n",
    "apple_sub_regions = apple_data.loc[apple_data['geo_type'] == 'sub-region']\n",
    "apple_counties = apple_data.loc[apple_data['geo_type'] == 'county']\n",
    "apple_cities = apple_data.loc[apple_data['geo_type'] == 'city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    John Hopkins Data Informal Documentation\n",
    "\"\"\"\n",
    "\n",
    "# Read in JHU time series data\n",
    "jhu_path = './data/COVID-19/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "jhu_data = pd.read_csv(jhu_path + 'time_series_covid19_confirmed_global.csv')\n",
    "\n",
    "# Rename US to United States in JHU Time Series df\n",
    "jhu_data.loc[jhu_data[\"Country/Region\"] == \"US\", \"Country/Region\"] = \"United States\"\n",
    "\n",
    "# Extract column names to be renamed\n",
    "# Date format does not match other data, so do conversion\n",
    "jhu_date_columns = jhu_data.loc[:, '1/22/20':]\n",
    "column_names = jhu_date_columns.columns\n",
    "updated_column_names = []\n",
    "\n",
    "# Convert column names to have matching date format\n",
    "for name in column_names:\n",
    "    date = datetime.strptime(name, '%m/%d/%y').strftime('%Y-%m-%d')\n",
    "    updated_column_names.append(date)\n",
    "\n",
    "# Update names and reform original DataFrame\n",
    "jhu_date_columns.columns = updated_column_names\n",
    "jhu_data = pd.concat([jhu_data.loc[:, :'Long'],\n",
    "                     jhu_date_columns],\n",
    "                     axis=1)\n",
    "\n",
    "# Forcibly clean up duplicate date columns to preserve memory\n",
    "del jhu_date_columns\n",
    "del column_names\n",
    "del updated_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DataFrames for each country\n",
    "country_df_list = []\n",
    "\n",
    "# Gets all countries in Apple's dataset\n",
    "for index, row in apple_countries.iterrows():\n",
    "    country_name = row['region'].strip()\n",
    "\n",
    "    # Flag for determining if matching country dataframe was found\n",
    "    found = False\n",
    "\n",
    "    # Iterates through list of country dataframes\n",
    "    for index, df in enumerate(country_df_list):\n",
    "        # Checks to determine if country is already present\n",
    "        if df['region'].iloc[0].strip() == country_name:\n",
    "            modified_df = country_df_list[index].append(row,\n",
    "                                                        ignore_index=True)\n",
    "            country_df_list[index] = modified_df\n",
    "            found = True\n",
    "\n",
    "    # Ensures that countries that were not already found are added\n",
    "    if not found:\n",
    "        country_df_list.append(row.to_frame().T)\n",
    "\n",
    "\n",
    "# Converts the \"direction type\" index label to be a more general \"datatype\"\n",
    "# This now indicates whether it was walking, driving, transit, or covid\n",
    "# Where covid data is JHU time series data, and all other data is apple maps\n",
    "# mobility statistics.\n",
    "for index, df in enumerate(country_df_list):\n",
    "    df.columns = ['datatype' if x == 'transportation_type'\n",
    "                  else x for x in df.columns]\n",
    "\n",
    "\n",
    "# Adds JHU data for each country into each country's dataframe\n",
    "for index, row in jhu_data.iterrows():\n",
    "    country_name = row['Country/Region'].strip()\n",
    "    subregion_name = str(row['Province/State']).strip()\n",
    "\n",
    "    # This step gets each row into a labeled format that is compatible\n",
    "    # with the dataframes in the country_df_list. This does not mean that the\n",
    "    # element counts will be compatible. Apple/Google are missing some days and\n",
    "    # JHU has more data available to it. The synchronization will\n",
    "    # need to be done in an additional for loop.\n",
    "    row = pd.concat([pd.Series(['country/region',\n",
    "                                row[1],\n",
    "                                'covid',\n",
    "                                subregion_name,\n",
    "                                country_name]),\n",
    "                    row['2020-01-22':'2020-09-21']],\n",
    "                    axis=0)\n",
    "\n",
    "    # Searches for matching country dataframe\n",
    "    for index, df in enumerate(country_df_list):\n",
    "        if df['region'].iloc[0].strip() == country_name:\n",
    "            new_index = ['geo_type',\n",
    "                         'region',\n",
    "                         'datatype',\n",
    "                         'sub-region',\n",
    "                         'country']\n",
    "            new_index.extend(list(row.index.values[5:]))\n",
    "            row.index = new_index\n",
    "            modified_df = country_df_list[index].append(row,\n",
    "                                                        ignore_index=True)\n",
    "            country_df_list[index] = modified_df\n",
    "\n",
    "# Filter out countries that are lacking covid data\n",
    "for index, df in enumerate(country_df_list):\n",
    "    try:\n",
    "        covid_data = df.loc[df['datatype'] == 'covid'].iloc[0].tolist()[5:]\n",
    "    except:\n",
    "        country_df_list.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df, val_df, test_df,\n",
    "                 label_columns=None):\n",
    "\n",
    "        # Training, validation, and testing dataframes\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "               enumerate(train_df.columns)}\n",
    "\n",
    "        # Parameters for window\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "          labels = tf.stack(\n",
    "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "            axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "              data=data,\n",
    "              targets=None,\n",
    "              sequence_length=self.total_window_size,\n",
    "              sequence_stride=1,\n",
    "              shuffle=True,\n",
    "              batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Generates Windows from data\n",
    "    \"\"\"\n",
    "    column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "    n = len(df)\n",
    "    train_df = df[0:int(n*0.7)]\n",
    "    val_df = df[int(n*0.7):int(n*0.9)]\n",
    "    test_df = df[int(n*0.9):]\n",
    "\n",
    "    # Perform Data Normalization\n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "\n",
    "    train_df = (train_df - train_mean) / train_std\n",
    "    val_df = (val_df - train_mean) / train_std\n",
    "    test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "\n",
    "    num_features = df.shape[1]\n",
    "\n",
    "    return WindowGenerator(input_width = 28,\n",
    "                           label_width = 1,\n",
    "                           shift = 28,\n",
    "                           train_df=train_df,\n",
    "                           val_df=val_df,\n",
    "                           test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window, patience=2, MAX_EPOCHS=30):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=patience,\n",
    "                                                      mode='min')\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.optimizers.Adam(),\n",
    "                  metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                        validation_data=window.val,\n",
    "                        callbacks=[early_stopping])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0       1      2\n",
      "2020-01-13     100     100      0\n",
      "2020-01-14    95.3  100.68      0\n",
      "2020-01-15  101.43   98.93      0\n",
      "2020-01-16    97.2   98.46      0\n",
      "2020-01-17  103.55  100.85      0\n",
      "...            ...     ...    ...\n",
      "2020-09-17  140.86  161.07  11948\n",
      "2020-09-18  151.82  165.59  12073\n",
      "2020-09-19  164.99  157.44  12226\n",
      "2020-09-20     160  141.41  12385\n",
      "2020-09-21   126.6  158.68  12535\n",
      "\n",
      "[253 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = country_df_list[0].transpose().fillna(0)[5:]\n",
    "print(df)\n",
    "w = preprocess_data(df)\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "gru_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.GRU(32, return_sequences=True),\n",
    "    tf.keras.layers.GRU(32, return_sequences=True),\n",
    "    tf.keras.layers.GRU(32, return_sequences=True),\n",
    "    tf.keras.layers.Dense(units=1000),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_and_fit(lstm_model, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compile_and_fit(gru_model, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
