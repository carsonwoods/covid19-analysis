{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid Mobility Data Analysis for Trend Prediction\n",
    "## Carson Woods, VZL837\n",
    "## CPSC 4180 Fall 2020 Final Project\n",
    "\n",
    "### Description:\n",
    "Machine Learning Analysis of Mobility Data and its impact on the rate of new COVID-19 Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from multiprocessing import Process\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Apple Mobility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Apple Mobility Data\n",
    "apple_data = pd.read_csv('./data/applemobilitytrends-2020-09-21.csv',\n",
    "                         low_memory=False)\n",
    "\n",
    "\n",
    "# Extract column names to be renamed\n",
    "apple_date_columns = apple_data.loc[:, '1/13/2020':]\n",
    "column_names = apple_date_columns.columns\n",
    "updated_column_names = []\n",
    "\n",
    "# Convert column names to have matching date format\n",
    "for name in column_names:\n",
    "    date = datetime.strptime(name, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "    updated_column_names.append(date)\n",
    "\n",
    "# Update names and reform original DataFrame\n",
    "apple_date_columns.columns = updated_column_names\n",
    "apple_data = pd.concat([apple_data.loc[:, :'country'], apple_date_columns],\n",
    "                       axis=1)\n",
    "\n",
    "# Forcibly clean up duplicate date columns to preserve memory\n",
    "del apple_date_columns\n",
    "del column_names\n",
    "del updated_column_names\n",
    "\n",
    "# Break the data into more specific subsets.\n",
    "# The data has the following structure (from broad to specific):\n",
    "# Country/Region -> Sub-Region(States in the US) -> County -> City\n",
    "apple_countries = apple_data.loc[apple_data['geo_type'] == 'country/region']\n",
    "apple_sub_regions = apple_data.loc[apple_data['geo_type'] == 'sub-region']\n",
    "apple_counties = apple_data.loc[apple_data['geo_type'] == 'county']\n",
    "apple_cities = apple_data.loc[apple_data['geo_type'] == 'city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Google Mobility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_region_code</th>\n",
       "      <th>country_region</th>\n",
       "      <th>sub_region_1</th>\n",
       "      <th>sub_region_2</th>\n",
       "      <th>metro_area</th>\n",
       "      <th>iso_3166_2_code</th>\n",
       "      <th>census_fips_code</th>\n",
       "      <th>date</th>\n",
       "      <th>retail_and_recreation_percent_change_from_baseline</th>\n",
       "      <th>grocery_and_pharmacy_percent_change_from_baseline</th>\n",
       "      <th>parks_percent_change_from_baseline</th>\n",
       "      <th>transit_stations_percent_change_from_baseline</th>\n",
       "      <th>workplaces_percent_change_from_baseline</th>\n",
       "      <th>residential_percent_change_from_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601813</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Kwekwe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601814</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Kwekwe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601815</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Kwekwe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601816</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Kwekwe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601817</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Midlands Province</td>\n",
       "      <td>Kwekwe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2601818 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        country_region_code        country_region       sub_region_1  \\\n",
       "0                        AE  United Arab Emirates                  0   \n",
       "1                        AE  United Arab Emirates                  0   \n",
       "2                        AE  United Arab Emirates                  0   \n",
       "3                        AE  United Arab Emirates                  0   \n",
       "4                        AE  United Arab Emirates                  0   \n",
       "...                     ...                   ...                ...   \n",
       "2601813                  ZW              Zimbabwe  Midlands Province   \n",
       "2601814                  ZW              Zimbabwe  Midlands Province   \n",
       "2601815                  ZW              Zimbabwe  Midlands Province   \n",
       "2601816                  ZW              Zimbabwe  Midlands Province   \n",
       "2601817                  ZW              Zimbabwe  Midlands Province   \n",
       "\n",
       "        sub_region_2 metro_area iso_3166_2_code  census_fips_code        date  \\\n",
       "0                  0          0               0               0.0  2020-02-15   \n",
       "1                  0          0               0               0.0  2020-02-16   \n",
       "2                  0          0               0               0.0  2020-02-17   \n",
       "3                  0          0               0               0.0  2020-02-18   \n",
       "4                  0          0               0               0.0  2020-02-19   \n",
       "...              ...        ...             ...               ...         ...   \n",
       "2601813       Kwekwe          0               0               0.0  2020-09-21   \n",
       "2601814       Kwekwe          0               0               0.0  2020-09-22   \n",
       "2601815       Kwekwe          0               0               0.0  2020-09-23   \n",
       "2601816       Kwekwe          0               0               0.0  2020-09-24   \n",
       "2601817       Kwekwe          0               0               0.0  2020-09-25   \n",
       "\n",
       "         retail_and_recreation_percent_change_from_baseline  \\\n",
       "0                                                      0.0    \n",
       "1                                                      1.0    \n",
       "2                                                     -1.0    \n",
       "3                                                     -2.0    \n",
       "4                                                     -2.0    \n",
       "...                                                    ...    \n",
       "2601813                                                0.0    \n",
       "2601814                                                0.0    \n",
       "2601815                                                0.0    \n",
       "2601816                                                0.0    \n",
       "2601817                                                0.0    \n",
       "\n",
       "         grocery_and_pharmacy_percent_change_from_baseline  \\\n",
       "0                                                      4.0   \n",
       "1                                                      4.0   \n",
       "2                                                      1.0   \n",
       "3                                                      1.0   \n",
       "4                                                      0.0   \n",
       "...                                                    ...   \n",
       "2601813                                                0.0   \n",
       "2601814                                                0.0   \n",
       "2601815                                                0.0   \n",
       "2601816                                                0.0   \n",
       "2601817                                                0.0   \n",
       "\n",
       "         parks_percent_change_from_baseline  \\\n",
       "0                                       5.0   \n",
       "1                                       4.0   \n",
       "2                                       5.0   \n",
       "3                                       5.0   \n",
       "4                                       4.0   \n",
       "...                                     ...   \n",
       "2601813                                 0.0   \n",
       "2601814                                 0.0   \n",
       "2601815                                 0.0   \n",
       "2601816                                 0.0   \n",
       "2601817                                 0.0   \n",
       "\n",
       "         transit_stations_percent_change_from_baseline  \\\n",
       "0                                                  0.0   \n",
       "1                                                  1.0   \n",
       "2                                                  1.0   \n",
       "3                                                  0.0   \n",
       "4                                                 -1.0   \n",
       "...                                                ...   \n",
       "2601813                                            0.0   \n",
       "2601814                                            0.0   \n",
       "2601815                                            0.0   \n",
       "2601816                                            0.0   \n",
       "2601817                                            0.0   \n",
       "\n",
       "         workplaces_percent_change_from_baseline  \\\n",
       "0                                            2.0   \n",
       "1                                            2.0   \n",
       "2                                            2.0   \n",
       "3                                            2.0   \n",
       "4                                            2.0   \n",
       "...                                          ...   \n",
       "2601813                                    -10.0   \n",
       "2601814                                     -3.0   \n",
       "2601815                                     -1.0   \n",
       "2601816                                     -3.0   \n",
       "2601817                                     -5.0   \n",
       "\n",
       "         residential_percent_change_from_baseline  \n",
       "0                                             1.0  \n",
       "1                                             1.0  \n",
       "2                                             1.0  \n",
       "3                                             1.0  \n",
       "4                                             1.0  \n",
       "...                                           ...  \n",
       "2601813                                       0.0  \n",
       "2601814                                       0.0  \n",
       "2601815                                       0.0  \n",
       "2601816                                       0.0  \n",
       "2601817                                       0.0  \n",
       "\n",
       "[2601818 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Google Data Informal Documentation\n",
    "\"\"\"\n",
    "\n",
    "# Read in Google Mobility Data\n",
    "google_data = pd.read_csv('./data/Google_Global_Mobility_Report.csv',\n",
    "                          low_memory=False).fillna(0)\n",
    "google_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load John Hopkins COVID-19 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in JHU time series data\n",
    "jhu_path = './data/COVID-19/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "jhu_data = pd.read_csv(jhu_path + 'time_series_covid19_confirmed_global.csv')\n",
    "\n",
    "# Rename US to United States in JHU Time Series df\n",
    "jhu_data.loc[jhu_data[\"Country/Region\"] == \"US\", \"Country/Region\"] = \"United States\"\n",
    "\n",
    "# Extract column names to be renamed\n",
    "# Date format does not match other data, so do conversion\n",
    "jhu_date_columns = jhu_data.loc[:, '1/22/20':]\n",
    "column_names = jhu_date_columns.columns\n",
    "updated_column_names = []\n",
    "\n",
    "# Convert column names to have matching date format\n",
    "for name in column_names:\n",
    "    date = datetime.strptime(name, '%m/%d/%y').strftime('%Y-%m-%d')\n",
    "    updated_column_names.append(date)\n",
    "\n",
    "# Update names and reform original DataFrame\n",
    "jhu_date_columns.columns = updated_column_names\n",
    "jhu_data = pd.concat([jhu_data.loc[:, :'Long'],\n",
    "                     jhu_date_columns],\n",
    "                     axis=1)\n",
    "\n",
    "# Forcibly clean up duplicate date columns to preserve memory\n",
    "del jhu_date_columns\n",
    "del column_names\n",
    "del updated_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Apple dataset into dataframes (one per country)\n",
    "This will allow for analysis to be done for every country more easily.\n",
    "Each dataframe will follow the same structure and format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DataFrames for each country\n",
    "country_df_list = []\n",
    "\n",
    "# Gets all countries in Apple's dataset\n",
    "for index, row in apple_countries.iterrows():\n",
    "    country_name = row['region'].strip()\n",
    "\n",
    "    # Flag for determining if matching country dataframe was found\n",
    "    found = False\n",
    "\n",
    "    # Iterates through list of country dataframes\n",
    "    for index, df in enumerate(country_df_list):\n",
    "        # Checks to determine if country is already present\n",
    "        if df['region'].iloc[0].strip() == country_name:\n",
    "            modified_df = country_df_list[index].append(row,\n",
    "                                                        ignore_index=True)\n",
    "            country_df_list[index] = modified_df\n",
    "            found = True\n",
    "\n",
    "    # Ensures that countries that were not already found are added\n",
    "    if not found:\n",
    "        country_df_list.append(row.to_frame().T)\n",
    "\n",
    "\n",
    "# Converts the \"direction type\" index label to be a more general \"datatype\"\n",
    "# This now indicates whether it was walking, driving, transit, or covid\n",
    "# Where covid data is JHU time series data, and all other data is apple maps\n",
    "# mobility statistics.\n",
    "for index, df in enumerate(country_df_list):\n",
    "    df.columns = ['datatype' if x == 'transportation_type'\n",
    "                  else x for x in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Google data to the previosuly generated list of country dataframes\n",
    "Iterates through Google's data and adds data for each country to the corresponding dataframe. \n",
    "If the country isn't found, the data is disregarded since I want to do analysis on country's with multiple data sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Google country data into dataframe\n",
    "for country in set(google_data['country_region'].to_list()):        \n",
    "    \n",
    "    # Second line ensures that no duplicate city data from countries is picked up\n",
    "    country_data = google_data.loc[google_data['country_region'] == country]\n",
    "    country_data = country_data.loc[country_data['sub_region_1'] == 0]\n",
    "\n",
    "    # Seperates description information from mobility data\n",
    "    # temp: stores mobility data\n",
    "    # country_data: stores description information\n",
    "    temp = country_data.transpose().iloc[7:]\n",
    "    country_data = country_data.transpose().iloc[:7]\n",
    "    country_data = country_data.iloc[:,:6]\n",
    "    country_data = country_data.transpose()\n",
    "\n",
    "    # creates a single column dataframe\n",
    "    # will be used to label dataframe within country dataframe\n",
    "    datatypes = temp.index.values.tolist()\n",
    "    datatypes = pd.DataFrame(datatypes, columns=['datatype'])\n",
    "\n",
    "    # renames column index in temp to use date format\n",
    "    # renames row indices to be numeric\n",
    "    # this makes concatenation work later\n",
    "    temp.columns = temp.iloc[0]\n",
    "    temp = temp.drop(temp.index[0])\n",
    "    temp.index = list(range(6))\n",
    "    datatypes = datatypes.drop(datatypes.index[0])\n",
    "    datatypes.index = list(range(6))\n",
    "\n",
    "    # creates country dataframe with all information\n",
    "    # additional logic is needed to match overall column index format\n",
    "    google_country_df = pd.concat([country_data, datatypes, temp], axis=1)\n",
    "    google_country_df.rename(columns={'country_region_code':'geo_type',\n",
    "                                      'country_region':'region',\n",
    "                                      'sub_region_1':'sub-region',\n",
    "                                      'sub_region_2':'country'}, inplace=True)\n",
    "\n",
    "    # reorder columns to match country_df\n",
    "    cols = list(google_country_df.columns.values)\n",
    "    cols_reorder = ['geo_type',\n",
    "                    'region',\n",
    "                    'datatype',\n",
    "                    'sub-region',\n",
    "                    'country']\n",
    "    cols = cols_reorder + cols[8:]\n",
    "    \n",
    "    google_country_df = google_country_df[cols].iloc[0:6]\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    google_country_df = google_country_df.fillna(0)\n",
    "    \n",
    "    \n",
    "    df = google_country_df.iloc[:, 5:]\n",
    "    \n",
    "    df = pd.concat([google_country_df.iloc[:, 0:6],\n",
    "                                   df.groupby(df.columns, axis=1).mean()],\n",
    "                                  axis=1)\n",
    "    \n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "    \n",
    "    # Normalize data to match apple dataset\n",
    "    numeric_cols = [col for col in df if df[col].dtype.kind != 'O']\n",
    "    df[numeric_cols] += 100\n",
    "    df['geo_type'] = \"country/region\"\n",
    "    df['region'] = country\n",
    "    google_country_df = df\n",
    "\n",
    "    # find matching country in country_df_list\n",
    "    # append google data to matching dataframe\n",
    "    for index, country_df in enumerate(country_df_list):\n",
    "        if country_df['region'].iloc[0].strip() == country.strip():\n",
    "            df = pd.concat([country_df, google_country_df], axis=0)\n",
    "            country_df_list[index] = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add JHU data to the previosuly generated list of country dataframes\n",
    "Iterates through JHU's data and adds data for each country to the corresponding dataframe. \n",
    "If the country isn't found, the data is disregarded since COVID data is required for any analysis to be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds JHU data for each country into each country's dataframe\n",
    "for index, row in jhu_data.iterrows():\n",
    "    country_name = row['Country/Region'].strip()\n",
    "    subregion_name = str(row['Province/State']).strip()\n",
    "\n",
    "    # This step gets each row into a labeled format that is compatible\n",
    "    # with the dataframes in the country_df_list. This does not mean that the\n",
    "    # element counts will be compatible. Apple/Google are missing some days and\n",
    "    # JHU has more data available to it. The synchronization will\n",
    "    # need to be done in an additional for loop.\n",
    "    row = pd.concat([pd.Series(['country/region',\n",
    "                                row[1],\n",
    "                                'covid',\n",
    "                                subregion_name,\n",
    "                                country_name]),\n",
    "                    row['2020-01-22':'2020-09-21']],\n",
    "                    axis=0)\n",
    "\n",
    "    # Searches for matching country dataframe\n",
    "    for index, df in enumerate(country_df_list):\n",
    "        if df['region'].iloc[0].strip() == country_name:\n",
    "            if subregion_name == \"nan\":\n",
    "                new_index = ['geo_type',\n",
    "                             'region',\n",
    "                             'datatype',\n",
    "                             'sub-region',\n",
    "                             'country']\n",
    "                new_index.extend(list(row.index.values[5:]))\n",
    "                row.index = new_index\n",
    "                modified_df = country_df_list[index].append(row,\n",
    "                                                            ignore_index=True)\n",
    "                country_df_list[index] = modified_df\n",
    "\n",
    "# Filter out countries that are lacking covid data\n",
    "for index, df in enumerate(country_df_list):\n",
    "    try:\n",
    "        covid_data = df.loc[df['datatype'] == 'covid'].iloc[0].tolist()[5:]\n",
    "    except:\n",
    "        country_df_list.pop(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WindowGenerator Class Definition\n",
    "Creates a WindowGenerator object that can generate sliding windows of training data for making small datasets of time series data more effective for training on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_df, val_df, test_df,\n",
    "                 label_columns=None):\n",
    "\n",
    "        # Training, validation, and testing dataframes\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "               enumerate(train_df.columns)}\n",
    "\n",
    "        # Parameters for window\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "          labels = tf.stack(\n",
    "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "            axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "              data=data,\n",
    "              targets=None,\n",
    "              sequence_length=self.total_window_size,\n",
    "              sequence_stride=1,\n",
    "              shuffle=True,\n",
    "              batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for ML\n",
    "Creates WindowGenerator for a dataframe of data and splits up the data. Also performs data normalization on dataset to ensure reasonable results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Generates Windows from data\n",
    "    \"\"\"\n",
    "    column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "    n = len(df)\n",
    "    train_df = df[0:int(n*0.8)]\n",
    "    val_df = df[int(n*0.5):int(n*0.7)]\n",
    "    test_df = df[int(n*0.7):]\n",
    "\n",
    "    # Perform Data Normalization\n",
    "    train_mean = train_df.mean()\n",
    "    train_std = train_df.std()\n",
    "\n",
    "    train_df = (train_df - train_mean) / train_std\n",
    "    val_df = (val_df - train_mean) / train_std\n",
    "    test_df = (test_df - train_mean) / train_std\n",
    "\n",
    "\n",
    "    num_features = df.shape[1]\n",
    "\n",
    "    return WindowGenerator(input_width = 18,\n",
    "                           label_width = 1,\n",
    "                           shift = 7,\n",
    "                           train_df=train_df,\n",
    "                           val_df=val_df,\n",
    "                           test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, window, patience=2, MAX_EPOCHS=30):\n",
    "\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.optimizers.Adam(),\n",
    "                  metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                        validation_data=window.val,\n",
    "                        verbose=0)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on country: Albania\n",
      "Training on country: Argentina\n",
      "Training on country: Austria\n",
      "Training on country: Belgium\n",
      "Training on country: Brazil\n",
      "Training on country: Bulgaria\n",
      "Training on country: Cambodia\n",
      "Training on country: Chile\n",
      "Training on country: Colombia\n",
      "Training on country: Croatia\n",
      "Training on country: Denmark\n",
      "Training on country: Egypt\n",
      "Training on country: Estonia\n",
      "Training on country: Finland\n",
      "Training on country: France\n",
      "Training on country: Germany\n",
      "Training on country: Greece\n",
      "Training on country: Hungary\n",
      "Training on country: Iceland\n",
      "Training on country: India\n",
      "Training on country: Indonesia\n",
      "Training on country: Ireland\n",
      "Training on country: Israel\n",
      "Training on country: Italy\n",
      "Training on country: Japan\n",
      "Training on country: Latvia\n",
      "Training on country: Lithuania\n",
      "Training on country: Luxembourg\n",
      "Training on country: Malaysia\n",
      "Training on country: Mexico\n",
      "Training on country: Morocco\n",
      "Training on country: Netherlands\n",
      "Training on country: New Zealand\n",
      "Training on country: Norway\n",
      "Training on country: Philippines\n",
      "Training on country: Poland\n",
      "Training on country: Portugal\n",
      "Training on country: Romania\n",
      "Training on country: Russia\n",
      "Training on country: Saudi Arabia\n",
      "Training on country: Serbia\n",
      "Training on country: Singapore\n",
      "Training on country: Slovakia\n",
      "Training on country: Slovenia\n",
      "Training on country: South Africa\n",
      "Training on country: Spain\n",
      "Training on country: Sweden\n",
      "Training on country: Switzerland\n",
      "Training on country: Thailand\n",
      "Training on country: Turkey\n",
      "Training on country: Ukraine\n",
      "Training on country: United Arab Emirates\n",
      "Training on country: United Kingdom\n",
      "Training on country: United States\n",
      "Training on country: Uruguay\n",
      "Training on country: Vietnam\n"
     ]
    }
   ],
   "source": [
    "# Ensures that figures directory exists\n",
    "# If figure regeneration is needed,\n",
    "# it ensures that the storage directory is regenerated\n",
    "results_path = os.path.join(os.getcwd(), 'results')\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(os.path.join(os.getcwd(),'results'))\n",
    "\n",
    "for df in country_df_list:\n",
    "\n",
    "    # Store country name for labeling\n",
    "    country_name = df['region'][0]\n",
    "\n",
    "    print(\"Training on country: \" + country_name)\n",
    "\n",
    "    # Ensures that NaN are set to 0\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    w = preprocess_data(df.transpose().fillna(0)[5:])\n",
    "\n",
    "    rnn_model = tf.keras.models.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "        tf.keras.layers.SimpleRNN(128),\n",
    "        #tf.keras.layers.GRU(32, return_sequences=True),\n",
    "        #tf.keras.layers.GRU(32, return_sequences=True),\n",
    "        #tf.keras.layers.Dense(units=1000),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    compile_and_fit(rnn_model, w, MAX_EPOCHS=50)\n",
    "    rnn_val_performance = str(rnn_model.evaluate(w.train, verbose=0))\n",
    "    rnn_performance = str(rnn_model.evaluate(w.test, verbose=0))\n",
    "\n",
    "    lstm_model = tf.keras.models.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        #tf.keras.layers.GRU(32, return_sequences=True),\n",
    "        #tf.keras.layers.GRU(32, return_sequences=True),\n",
    "        #tf.keras.layers.Dense(units=1000),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    compile_and_fit(lstm_model, w, MAX_EPOCHS=500)\n",
    "    lstm_val_performance = str(lstm_model.evaluate(w.train, verbose=0))\n",
    "    lstm_performance = str(lstm_model.evaluate(w.test, verbose=0))\n",
    "\n",
    "    gru_model = tf.keras.models.Sequential([\n",
    "        # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "        tf.keras.layers.GRU(32, return_sequences=True),\n",
    "        #tf.keras.layers.GRU(32, return_sequences=True),\n",
    "        #tf.keras.layers.GRU(32, return_sequences=True),\n",
    "        #tf.keras.layers.Dense(units=1000),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    compile_and_fit(gru_model, w, MAX_EPOCHS=500)\n",
    "    gru_val_performance = str(gru_model.evaluate(w.train, verbose=0))\n",
    "    gru_performance = str(gru_model.evaluate(w.test, verbose=0))\n",
    "\n",
    "    # Ensures that there is a path for figures to be stored (per country)\n",
    "    country_path = os.path.join(results_path, country_name)\n",
    "    if not os.path.exists(country_path):\n",
    "        os.makedirs(country_path)\n",
    "\n",
    "    model_performance_file = open(country_path + \"/\" + country_name + \"_model_performance.txt\", \"w+\")\n",
    "\n",
    "    model_performance_file.write(\"Results Format: \" + str(gru_model.metrics_names) + \"\\n\")\n",
    "\n",
    "    model_performance_file.write(\"RNN_MODEL:\\n\")\n",
    "    model_performance_file.write(\"Val Performance: \" + rnn_val_performance + \"\\n\" )\n",
    "    model_performance_file.write(\"Performance: \" + rnn_performance + \"\\n\\n\")\n",
    "\n",
    "    model_performance_file.write(\"LSTM_MODEL:\\n\")\n",
    "    model_performance_file.write(\"Val Performance: \" + lstm_val_performance + \"\\n\" )\n",
    "    model_performance_file.write(\"Performance: \" + lstm_performance + \"\\n\\n\")\n",
    "\n",
    "    model_performance_file.write(\"GRU_MODEL:\\n\")\n",
    "    model_performance_file.write(\"Val Performance: \" + gru_val_performance + \"\\n\" )\n",
    "    model_performance_file.write(\"Performance: \" + gru_performance)\n",
    "\n",
    "    model_performance_file.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
